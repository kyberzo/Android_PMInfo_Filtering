# Dockerfile for Transformer training

FROM python:3.8-slim

LABEL model="Transformer"
LABEL description="Transformer with multi-head attention for package classification"

WORKDIR /workspace

# Install dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

RUN pip install --no-cache-dir \
    protobuf==3.20.3 \
    tensorflow==2.8.0 \
    keras==2.8.0 \
    numpy==1.22.0 \
    scikit-learn==1.0.2 \
    pandas==1.4.0

# Copy training scripts
COPY data_utils.py /workspace/
COPY train_transformer.py /workspace/

# Create output directory
RUN mkdir -p /workspace/output

# Set environment variables
ENV PYTHONUNBUFFERED=1
ENV TF_CPP_MIN_LOG_LEVEL=2

# Default training parameters
ENV EPOCHS=100
ENV BATCH_SIZE=1024
ENV PATIENCE=8
ENV D_MODEL=128
ENV NUM_HEADS=4
ENV FF_DIM=256
ENV NUM_BLOCKS=2
ENV DROPOUT=0.3

# Entry point
ENTRYPOINT ["python3", "train_transformer.py"]

# Default arguments
CMD ["--epochs", "${EPOCHS}", \
     "--batch-size", "${BATCH_SIZE}", \
     "--patience", "${PATIENCE}", \
     "--d-model", "${D_MODEL}", \
     "--num-heads", "${NUM_HEADS}", \
     "--ff-dim", "${FF_DIM}", \
     "--num-blocks", "${NUM_BLOCKS}", \
     "--dropout", "${DROPOUT}"]
